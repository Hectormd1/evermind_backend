# -*- coding: utf-8 -*-
# pylint: disable=import-error,unused-import,broad-except,too-many-locals
# type: ignore
"""
Evermind AI Backend - FastAPI + Gradio
Backend para transcripci√≥n de audio y chat con IA
"""

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import whisper
import tempfile
import os
import traceback
import random
import httpx
import gc  # Para garbage collection manual
import psutil  # Para monitoreo de memoria
import time  # Para timestamps
from typing import List, Optional
from dotenv import load_dotenv
import asyncio

# Imports opcionales para HF Spaces
try:
    import gradio as gr  # type: ignore
    GRADIO_AVAILABLE = True
except ImportError:
    GRADIO_AVAILABLE = False
    print("‚ö†Ô∏è Gradio no disponible - funcionando solo como API")

try:
    import torch  # type: ignore
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# Cargar variables de entorno
load_dotenv()

# Log de inicio

print("üöÄ EVERMIND BACKEND: Iniciado.")
print(f"üíæ MEMORIA INICIAL: {psutil.virtual_memory().percent}%")

app = FastAPI(title="Evermind AI Backend", version="1.0.0")

# Agregar middleware CORS para React Native
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Carga el modelo base de Whisper solo cuando sea necesario (lazy loading)
model = None

def load_whisper_model():
    global model
    if model is None:
        try:
            import whisper
            print("ü§ñ WHISPER: Cargando modelo 'small'...")
            
            # Usar modelo 'small' - menor latencia y consumo de memoria
            model = whisper.load_model("small", device="cpu", download_root=None)
            
            print("‚úÖ WHISPER: Modelo 'small' cargado en CPU")
            
            # Limpiar memoria inmediatamente despu√©s de cargar
            gc.collect()
            
        except Exception as e:
            print(f"‚ùå WHISPER ERROR: {e}")
            model = False
    return model

# Funci√≥n mejorada para liberar memoria despu√©s de transcripci√≥n
def cleanup_whisper_memory():
    gc.collect()  # Garbage collection inmediato
    # Si torch est√° disponible, limpiar cache
    if TORCH_AVAILABLE:
        try:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except Exception:
            pass
    memory_percent = psutil.virtual_memory().percent
    # Si el uso de memoria es muy alto, forzar limpieza m√°s agresiva
    if memory_percent > 80:
        for i in range(3):
            gc.collect()

# Funci√≥n para verificar memoria disponible
def check_memory_status():
    memory = psutil.virtual_memory()
    return {
        "used_percent": memory.percent,
        "available_mb": memory.available // (1024 * 1024),
        "total_mb": memory.total // (1024 * 1024)
    }

# Configuraci√≥n de IA - Proveedores funcionales
TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY", "")
TOGETHER_MODEL = os.getenv("TOGETHER_MODEL", "mistralai/Mixtral-8x7B-Instruct-v0.1")

# OpenRouter - modelos gratuitos disponibles
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY", "")  # Opcional para algunos modelos
OPENROUTER_MODEL = "mistralai/mistral-7b-instruct:free"  # Modelo completamente gratuito

# Groq (API gratuita muy r√°pida) - Principal proveedor
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")
GROQ_MODEL = os.getenv("GROQ_MODEL", "openai/gpt-oss-120b")

# Modelos para el chat
class ChatMessage(BaseModel):
    role: str
    content: str

class ReflectRequest(BaseModel):
    mood_before: Optional[int] = None
    messages: List[ChatMessage] = []
    locale: str = "es-ES"

# IA real usando m√∫ltiples proveedores gratuitos
async def call_together_ai(messages: List[dict]) -> str:
    """Llama a Together AI"""
    if not TOGETHER_API_KEY:
        return None
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api.together.xyz/v1/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {TOGETHER_API_KEY}",
                },
                json={
                    "model": TOGETHER_MODEL,
                    "messages": messages,
                    "max_tokens": 380,
                    "temperature": 0.7,
                    "top_p": 0.95,
                },
                timeout=30.0
            )
            
            if response.status_code == 200:
                data = response.json()
                reply = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                if reply and len(reply) > 40:
                    return reply.strip()
            else:
                print(f"‚ùå Together AI error: {response.status_code}")
                return None
    except Exception as e:
        print(f"‚ùå Together AI exception: {e}")
        return None

async def call_openrouter_ai(messages: List[dict]) -> str:
    """Llama a OpenRouter con modelos gratuitos"""
    try:
        headers = {
            "Content-Type": "application/json",
            "HTTP-Referer": "https://evermind.app",
            "X-Title": "Evermind"
        }
        
        # API key es opcional para modelos gratuitos
        if OPENROUTER_API_KEY:
            headers["Authorization"] = f"Bearer {OPENROUTER_API_KEY}"
        
        async with httpx.AsyncClient(timeout=60.0) as client:  # Timeout m√°s largo
            response = await client.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers=headers,
                json={
                    "model": OPENROUTER_MODEL,
                    "messages": messages,
                    "max_tokens": 380,
                    "temperature": 0.7,
                },
                timeout=60.0  # Timeout espec√≠fico de 60 segundos
            )
            
            if response.status_code == 200:
                data = response.json()
                reply = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                if reply and len(reply) > 20:
                    return reply.strip()
            else:
                error_text = response.text
                print(f"‚ùå OpenRouter error: {response.status_code} - {error_text}")
                return None
    except asyncio.TimeoutError:
        print("‚ùå OpenRouter timeout: >60s")
        return None
    except Exception as e:
        print(f"‚ùå OpenRouter exception: {e}")
        return None

async def call_groq_ai(messages: List[dict]) -> str:
    """Llama a Groq AI (muy r√°pido y gratuito)"""
    if not GROQ_API_KEY:
        return None
    
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:  # Timeout m√°s largo
            response = await client.post(
                "https://api.groq.com/openai/v1/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {GROQ_API_KEY}",
                },
                json={
                    "model": GROQ_MODEL,
                    "messages": messages,
                    "max_tokens": 380,
                    "temperature": 0.7,
                },
                timeout=60.0  # Timeout espec√≠fico de 60 segundos
            )
            
            if response.status_code == 200:
                data = response.json()
                reply = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                if reply and len(reply) > 40:
                    return reply.strip()
            else:
                print(f"‚ùå Groq error: {response.status_code} - {response.text}")
                return None
    except asyncio.TimeoutError:
        print("‚ùå Groq timeout: >60s")
        return None
    except Exception as e:
        print(f"‚ùå Groq exception: {e}")
        return None

async def generate_ai_response(messages: List[ChatMessage], mood_before: Optional[int] = None) -> str:
    """Genera respuestas reales de IA usando m√∫ltiples proveedores gratuitos como fallback"""
    
    # Construir mensajes para las APIs
    ai_messages = []
    
    # System prompt optimizado para respuestas coherentes y naturales
    ai_messages.append({
        "role": "system",
        "content": "Eres un psic√≥logo y acompa√±ante emocional emp√°tico llamado Evermind. Tu objetivo es ayudar a las personas a reflexionar sobre sus emociones y sentimientos de manera natural y humana. REGLAS IMPORTANTES: 1) LEE CUIDADOSAMENTE lo que el usuario te dice antes de responder. 2) Responde SOLO en espa√±ol natural de Espa√±a, sin anglicismos. 3) S√© espec√≠fico y relevante a lo que el usuario menciona. 4) Si el usuario expresa frustraci√≥n, tristeza, enojo o cualquier emoci√≥n espec√≠fica, recon√≥cela directamente. 5) Haz preguntas abiertas para que la persona profundice en sus emociones. 6) No des consejos gen√©ricos, personaliza tu respuesta a su situaci√≥n espec√≠fica. 7) M√°ximo 150 palabras. 8) Habla como un amigo comprensivo, no como un terapeuta formal."
    })
    
    # Agregar historial de mensajes
    for msg in messages:
        if msg.role in ['user', 'assistant']:
            ai_messages.append({
                "role": msg.role,
                "content": msg.content
            })
    
    # Probar proveedores en orden: Groq (principal) -> OpenRouter -> Together AI (sin cr√©ditos)
    providers = [
        ("Groq AI", call_groq_ai),
        ("OpenRouter", call_openrouter_ai),
        ("Together AI", call_together_ai),
    ]
    
    for provider_name, provider_func in providers:
        try:
            print(f"ü§ñ Intentando {provider_name}...")
            reply = await provider_func(ai_messages)
            
            if reply and len(reply) > 20:  # Validar que la respuesta sea √∫til
                print(f"‚úÖ {provider_name} respondi√≥ exitosamente")
                return reply
            else:
                print(f"‚ö†Ô∏è {provider_name} respuesta muy corta o vac√≠a")
                
        except Exception as e:
            print(f"‚ùå Error en {provider_name}: {e}")
            continue
    
    # Si todos fallan, responder que no se puede procesar la solicitud
    print("‚ùå Todos los proveedores de IA fallaron")
    return "Lo siento, no puedo generar una respuesta en este momento. Por favor, int√©ntalo de nuevo m√°s tarde."

def generate_simple_fallback() -> str:
    """Respuesta simple cuando todas las IAs fallan"""
    return "Lo siento, no puedo generar una respuesta en este momento. Por favor, int√©ntalo de nuevo m√°s tarde."

# Endpoint de transcripci√≥n habilitado
@app.post("/transcribe")
async def transcribe_audio(file: UploadFile = File(...)):
    """Transcribir audio usando Whisper"""
    global model
    
    # Log de inicio inmediato
    print("üéµ TRANSCRIPCI√ìN: Procesando audio desde React Native")
    
    # Cargar modelo si no est√° cargado (esto puede tomar tiempo la primera vez)
    if model is None or model is False:
        print("ü§ñ WHISPER: Cargando modelo bajo demanda...")
        load_whisper_model()
    
    if not model:
        print("‚ùå WHISPER: Modelo no disponible")
        raise HTTPException(status_code=503, detail="Whisper no est√° disponible en este momento")
    
    if not file:
        raise HTTPException(status_code=400, detail="No se recibi√≥ archivo de audio")
    
    try:
        # Crear archivo temporal
        with tempfile.NamedTemporaryFile(delete=False, suffix=".m4a") as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        print(f"üéµ TRANSCRIPCI√ìN: Procesando audio desde React Native")
        print(f"üìÅ ARCHIVO: {temp_file_path}")
        print(f"üíæ MEMORIA PRE-TRANSCRIPCI√ìN: {psutil.virtual_memory().percent}%")
        
        # Transcribir con Whisper (configuraci√≥n de M√ÅXIMA PRECISI√ìN para espa√±ol)
        result = model.transcribe(
            temp_file_path,
            language="es",  # Forzar espa√±ol
            fp16=False,  # Mejor compatibilidad
            temperature=0.0,  # M√°xima precisi√≥n determin√≠stica
            word_timestamps=False,  # Desactivar para velocidad
            no_speech_threshold=0.5,  # M√°s sensible para detectar habla
            logprob_threshold=-1.0,  # Balanceado con la confianza
            compression_ratio_threshold=2.4,  # Evitar repeticiones
            condition_on_previous_text=False,  # Desactivar para independencia
            initial_prompt="Transcripci√≥n precisa en espa√±ol de reflexi√≥n personal sobre trabajo y vida profesional.",  # Contexto espec√≠fico
            beam_size=5,  # Mayor b√∫squeda para m√°xima precisi√≥n
            best_of=3,  # Tres candidatos para mejor resultado
            patience=2.0  # M√°s paciente para mejor calidad
        )
        
        # ‚≠ê LIMPIEZA INMEDIATA Y AGRESIVA DE MEMORIA DESPU√âS DE TRANSCRIPCI√ìN
        print("üßπ LIMPIEZA: Liberando memoria post-transcripci√≥n...")
        cleanup_whisper_memory()
        
        # Limpiar archivo temporal inmediatamente
        try:
            os.unlink(temp_file_path)
            print("üóëÔ∏è ARCHIVO TEMPORAL: Eliminado exitosamente")
        except Exception as cleanup_error:
            print(f"‚ö†Ô∏è ARCHIVO TEMPORAL: Error al eliminar - {cleanup_error}")
        
        print(f"üíæ MEMORIA POST-LIMPIEZA: {psutil.virtual_memory().percent}%")
        
        transcribed_text = result["text"].strip()
        
        print(f"üìù RESULTADO: '{transcribed_text}'")
        
        if not transcribed_text:
            return {"text": "No se pudo transcribir el audio"}
        
        return {"text": transcribed_text}
        
    except Exception as e:
        print(f"‚ùå Error transcribiendo: {e}")
        traceback.print_exc()
        
        # Limpiar archivo temporal si existe
        try:
            if 'temp_file_path' in locals():
                os.unlink(temp_file_path)
        except:
            pass
            
        raise HTTPException(status_code=500, detail=f"Error al transcribir: {str(e)}")

@app.post("/reflect")
async def reflect_chat(request: ReflectRequest):
    """Endpoint para generar respuestas de IA real"""
    try:
        print(f"ü§ñ Generando respuesta IA real para {len(request.messages)} mensajes")
        # Respuesta general de la IA
        reply = await generate_ai_response(request.messages, request.mood_before)

        # Prompt espec√≠fico para microacci√≥n breve y relevante
        microaction_prompt = (
            "A partir de la conversaci√≥n previa, sugiere SOLO UNA microacci√≥n breve, concreta y accionable que ayude a la persona a mejorar su bienestar emocional. "
            "No repitas consejos gen√©ricos, personaliza la microacci√≥n seg√∫n lo que la persona ha contado. "
            "Ejemplo: 'Da un paseo de 5 minutos', 'Escribe 3 cosas buenas de tu d√≠a', 'Env√≠a un mensaje a un amigo', etc. (No tienes porque usar estas frases, solo son ejemplos de que tipo de consejo queremos)"
            "Devuelve SOLO la microacci√≥n, sin explicaciones ni saludos."
        )
        # Construir historial para la IA: toda la conversaci√≥n + prompt de microacci√≥n
        microaction_messages = [
            {"role": "system", "content": microaction_prompt}
        ]
        for msg in request.messages:
            microaction_messages.append({"role": msg.role, "content": msg.content})
        # √öltimo mensaje: "¬øQu√© microacci√≥n concreta me propones?"
        microaction_messages.append({"role": "user", "content": "¬øQu√© microacci√≥n concreta me propones?"})

        # Usar el mismo generador de IA pero con el prompt de microacci√≥n
        step_suggestion = await generate_ai_response([
            ChatMessage(role=m["role"], content=m["content"]) for m in microaction_messages
        ])

        # Limpiar la microacci√≥n: solo la primera l√≠nea, sin saludos ni explicaciones
        if step_suggestion:
            step_suggestion = step_suggestion.strip().split("\n")[0]
            # Quitar saludos o frases largas
            for prefix in ["Te propongo ", "¬øQu√© te parece si ", "Podr√≠as intentar ", "Te animo a "]:
                if step_suggestion.lower().startswith(prefix.lower()):
                    step_suggestion = step_suggestion[len(prefix):].strip()

        return {
            "reply": reply,
            "step_suggestion": step_suggestion,
            "provider": "multi_provider_system"
        }
    except Exception as e:
        print("\n--- ERROR EN /reflect ---")
        traceback.print_exc()
        print("---------------------------\n")
        return JSONResponse(status_code=500, content={"error": str(e)})

@app.get("/")
@app.head("/")
def root():
    # Log cuando HF Spaces hace la petici√≥n autom√°tica
    print("üîÑ KEEP-ALIVE: Petici√≥n recibida desde Hugging Face Spaces (HEAD /)")
    
    return {
        "status": "ok", 
        "service": "Evermind AI Backend",
        "version": "1.0.0",
        "whisper_loaded": model is not None and model is not False,
        "endpoints": ["/transcribe", "/reflect", "/check-credits", "/providers-status", "/health", "/ping"], 
        "ai_providers": ["groq", "openrouter", "together"]
    }

from fastapi import Request

@app.get("/ping")
@app.head("/ping")  # ‚≠ê SOPORTE PARA HEAD REQUEST
def ping(request: Request):
    """Endpoint simple para mantener el servicio activo en HF Spaces"""
    user_agent = request.headers.get('user-agent', '')
    user_agent_lower = user_agent.lower()
    if 'cloudflare' in user_agent_lower:
        caller = 'cloudflare_worker'
    elif 'huggingface' in user_agent_lower or 'python-httpx' in user_agent_lower:
        caller = 'huggingface_spaces_auto'
    else:
        caller = 'unknown'
    print(f"üîÑ KEEP-ALIVE: Ping recibido desde {caller}")
    print(f"   ‚Ü≥ User-Agent: {user_agent}")
    print(f"‚è∞ Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S UTC', time.gmtime())}")
    cleanup_whisper_memory()
    return {
        "status": "pong",
        "timestamp": int(time.time()),
        "service": "evermind-backend",
        "memory_usage": "optimized",
        "keep_alive": "active",
        "source": caller,
        "user_agent": user_agent
    }

@app.get("/health")
@app.head("/health")  # ‚≠ê ENDPOINT DE SALUD OPTIMIZADO
def health_check():
    """Endpoint de salud con monitoreo de memoria"""
    
    try:
        memory_info = check_memory_status()
        
        print(f"üè• HEALTH CHECK: Memoria en uso: {memory_info['used_percent']}%")
        
        return {
            "status": "healthy",
            "timestamp": int(time.time()),
            "memory": memory_info,
            "whisper_loaded": model is not None and model is not False,
            "service_active": True,
            "version": "3.0-optimized",
            "render_service_id": os.environ.get("RENDER_SERVICE_ID", "local")
        }
    except Exception as e:
        print(f"‚ùå HEALTH CHECK ERROR: {e}")
        return {
            "status": "healthy",
            "timestamp": int(time.time()),
            "service_active": True,
            "error": str(e)
        }

@app.get("/status")
@app.head("/status")  # ‚≠ê NUEVO ENDPOINT DE STATUS COMPLETO
def status_check():
    """Endpoint completo de status para keep-alive ultra-agresivo"""
    
    try:
        memory_info = check_memory_status()
        
        # Si la memoria est√° muy alta, ejecutar limpieza
        if memory_info['used_percent'] > 75:
            print("‚ö†Ô∏è MEMORIA ALTA DETECTADA: Ejecutando limpieza autom√°tica...")
            cleanup_whisper_memory()
            memory_info = check_memory_status()  # Actualizar despu√©s de limpieza
        
        print(f"üìä STATUS CHECK: Servicio activo - Memoria: {memory_info['used_percent']}%")
        
        return {
            "status": "fully_active",
            "timestamp": int(time.time()),
            "uptime": "continuous",
            "memory": memory_info,
            "whisper_model": "base" if model and model is not False else "not_loaded",
            "endpoints_active": ["/transcribe", "/reflect", "/ping", "/health", "/status"],
            "keep_alive_mode": "ultra_aggressive",
            "auto_cleanup": memory_info['used_percent'] <= 75
        }
    except Exception as e:
        print(f"‚ùå STATUS CHECK ERROR: {e}")
        return {
            "status": "active_with_errors",
            "timestamp": int(time.time()),
            "error": str(e)
        }

@app.get("/providers-status")
async def providers_status():
    """Verificar qu√© proveedores est√°n configurados"""
    status = {
        "groq_ai": {
            "configured": bool(GROQ_API_KEY),
            "model": GROQ_MODEL if GROQ_API_KEY else "No configurado",
            "priority": 1
        },
        "openrouter": {
            "configured": bool(OPENROUTER_API_KEY),
            "model": OPENROUTER_MODEL,
            "note": "Modelo gratuito - API key opcional",
            "priority": 2
        },
        "together_ai": {
            "configured": bool(TOGETHER_API_KEY),
            "model": TOGETHER_MODEL if TOGETHER_API_KEY else "No configurado",
            "note": "Sin cr√©ditos actualmente",
            "priority": 3
        }
    }
    
    configured_count = sum(1 for p in status.values() if p["configured"])
    
    return {
        "providers": status,
        "configured_providers": configured_count,
        "total_providers": len(status),
        "recommendation": "Sistema funcionando con Groq AI (principal) y OpenRouter (fallback)"
    }

@app.get("/check-credits")
async def check_credits():
    """Verificar el estado de los cr√©ditos de Together AI"""
    if not TOGETHER_API_KEY:
        return {"status": "no_api_key", "fallback": True}
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api.together.xyz/v1/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {TOGETHER_API_KEY}",
                },
                json={
                    "model": TOGETHER_MODEL,
                    "messages": [{"role": "user", "content": "Hi"}],
                    "max_tokens": 5,
                },
                timeout=10.0
            )
            
            if response.status_code == 200:
                return {"status": "ok", "credits_available": True, "fallback": False}
            elif response.status_code == 402:
                return {"status": "no_credits", "credits_available": False, "fallback": True}
            else:
                return {"status": "error", "code": response.status_code, "fallback": True}
                
    except Exception as e:
        return {"status": "connection_error", "error": str(e), "fallback": True}

@app.post("/test-together")
async def test_together():
    """Probar Together AI individualmente"""
    test_messages = [{"role": "user", "content": "Hola, prueba r√°pida"}]
    result = await call_together_ai(test_messages)
    return {"provider": "Together AI", "result": result, "status": "success" if result else "failed"}

@app.post("/test-groq")
async def test_groq():
    """Probar Groq AI individualmente"""
    test_messages = [{"role": "user", "content": "Hola, prueba r√°pida"}]
    result = await call_groq_ai(test_messages)
    return {"provider": "Groq AI", "result": result, "status": "success" if result else "failed"}

@app.post("/test-openrouter")
async def test_openrouter():
    """Probar OpenRouter individualmente"""
    test_messages = [{"role": "user", "content": "Hola, prueba r√°pida"}]
    result = await call_openrouter_ai(test_messages)
    return {"provider": "OpenRouter", "result": result, "status": "success" if result else "failed"}

# =============================================================================
# INTERFAZ GRADIO PARA HUGGING FACE SPACES (Solo si est√° disponible)
# =============================================================================

if GRADIO_AVAILABLE:
    
    def transcribe_audio_for_gradio(audio_file):
        """Funci√≥n para transcribir audio desde Gradio con configuraci√≥n ultra-precisa"""
        try:
            load_whisper_model()
            if audio_file is None:
                return "‚ùå No se ha proporcionado un archivo de audio"
            
            # Transcribir usando Whisper con configuraci√≥n optimizada para velocidad
            result = model.transcribe(
                audio_file, 
                language="es",
                temperature=0.0,  # M√°xima precisi√≥n
                word_timestamps=False,  # Desactivar para velocidad
                no_speech_threshold=0.6,  # Balanceado
                logprob_threshold=-1.0,  # Balanceado con la confianza
                compression_ratio_threshold=2.4,  # Evitar repeticiones
                condition_on_previous_text=False,  # Desactivar para velocidad
                initial_prompt=None,  # Sin prompt para velocidad
                beam_size=1,  # M√≠nimo para m√°xima velocidad
                best_of=1,  # M√≠nimo para m√°xima velocidad
                patience=1.0  # Menos paciente para mayor velocidad
            )
            transcription = result["text"].strip()
            
            if not transcription:
                return "‚ùå No se pudo transcribir el audio"
            
            return f"‚úÖ Transcripci√≥n: {transcription}"
        
        except Exception as e:
            return f"‚ùå Error en transcripci√≥n: {str(e)}"

    async def chat_for_gradio(message, history):
        """Funci√≥n para chat desde Gradio"""
        try:
            # Convertir historial de Gradio (formato messages) a formato de mensajes API
            messages = []
            
            # Si history viene en formato messages (nuevo formato Gradio)
            if history and isinstance(history, list) and len(history) > 0:
                if isinstance(history[0], dict) and "role" in history[0]:
                    # Ya est√° en formato messages
                    messages = history.copy()
                else:
                    # Formato tuplas (viejo formato)
                    for human, assistant in history:
                        messages.append({"role": "user", "content": human})
                        if assistant:
                            messages.append({"role": "assistant", "content": assistant})
            
            # Agregar mensaje actual
            messages.append({"role": "user", "content": message})
            
            # Convertir a formato ChatMessage para la funci√≥n existente
            class ChatMessage(BaseModel):
                role: str
                content: str
            
            chat_messages = [ChatMessage(role=msg["role"], content=msg["content"]) for msg in messages]
            
            # Llamar a la funci√≥n de chat existente
            response = await generate_ai_response(chat_messages)
            
            return response
        
        except Exception as e:
            return f"‚ùå Error en chat: {str(e)}"

    # Crear interfaz Gradio
    with gr.Blocks(title="Evermind AI Backend", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# üß† Evermind AI Backend")
        gr.Markdown("Backend para transcripci√≥n de audio y chat con IA")
        
        with gr.Tab("üé§ Transcripci√≥n de Audio"):
            gr.Markdown("### Sube un archivo de audio para transcribir")
            audio_input = gr.Audio(
                label="Archivo de Audio",
                type="filepath",
                format="wav"
            )
            transcribe_btn = gr.Button("üéØ Transcribir", variant="primary")
            transcription_output = gr.Textbox(
                label="Transcripci√≥n",
                lines=3,
                placeholder="La transcripci√≥n aparecer√° aqu√≠..."
            )
            
            transcribe_btn.click(
                fn=transcribe_audio_for_gradio,
                inputs=[audio_input],
                outputs=[transcription_output]
            )
        
        with gr.Tab("üí¨ Chat con IA"):
            gr.Markdown("### Chatea con la IA de Evermind")
            chatbot = gr.Chatbot(
                label="Conversaci√≥n",
                height=400,
                placeholder="Inicia una conversaci√≥n...",
                type="messages"
            )
            msg = gr.Textbox(
                label="Mensaje",
                placeholder="Escribe tu mensaje aqu√≠...",
                lines=2
            )
            send_btn = gr.Button("üì§ Enviar", variant="primary")
            clear_btn = gr.Button("üóëÔ∏è Limpiar Chat", variant="secondary")
            
            def respond(message, history):
                # Funci√≥n wrapper para manejar el async con nuevo formato messages
                if not message.strip():
                    return history, ""
                    
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    response = loop.run_until_complete(chat_for_gradio(message, history))
                    
                    # Agregar mensaje y respuesta al historial
                    new_message = {"role": "user", "content": message}
                    new_response = {"role": "assistant", "content": response}
                    
                    if history is None:
                        history = []
                    
                    history.append(new_message)
                    history.append(new_response)
                    
                    return history, ""
                except Exception as e:
                    error_response = {"role": "assistant", "content": f"‚ùå Error: {str(e)}"}
                    if history is None:
                        history = []
                    history.append({"role": "user", "content": message})
                    history.append(error_response)
                    return history, ""
                finally:
                    loop.close()
            
            send_btn.click(
                fn=respond,
                inputs=[msg, chatbot],
                outputs=[chatbot, msg]
            )
            
            clear_btn.click(
                fn=lambda: ([], ""),
                outputs=[chatbot, msg]
            )
        
        with gr.Tab("üìä Estado del Sistema"):
            gr.Markdown("### Informaci√≥n del backend")
            status_output = gr.Textbox(
                label="Estado",
                value="‚úÖ Backend funcionando correctamente",
                interactive=False
            )
            refresh_btn = gr.Button("üîÑ Actualizar Estado")
            
            def get_system_status():
                try:
                    memory_percent = psutil.virtual_memory().percent
                    model_status = "‚úÖ Cargado" if model else "‚è≥ No cargado"
                    return f"""
‚úÖ **Backend Status**: Activo
ü§ñ **Modelo Whisper**: {model_status}
üíæ **Uso de Memoria**: {memory_percent}%
üåê **Endpoints**: /transcribe, /chat, /ping
                    """
                except Exception as e:
                    return f"‚ùå Error obteniendo estado: {str(e)}"
            
            refresh_btn.click(
                fn=get_system_status,
                outputs=[status_output]
            )

    # Montar la aplicaci√≥n Gradio en FastAPI
    app = gr.mount_gradio_app(app, demo, path="/")
    print("‚úÖ GRADIO: Interfaz montada en /")

else:
    print("‚ö†Ô∏è GRADIO: No disponible - funcionando solo como API REST")

# Debug final para HF Spaces
print("üîç DEBUG: Aplicaci√≥n FastAPI inicializada correctamente")
print(f"üîç DEBUG: Gradio disponible: {GRADIO_AVAILABLE}")
print(f"üîç DEBUG: Tipo de app: {type(app)}")

# HF Spaces ejecutar√° autom√°ticamente la aplicaci√≥n
# No ejecutar uvicorn.run() cuando se importa desde app.py
